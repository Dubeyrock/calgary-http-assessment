{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3551217",
   "metadata": {},
   "source": [
    "# Web Server Log Analysis - Python Take-Home Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb0efa",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This assessment involves analyzing the Calgary HTTP dataset, which contains approximately one year's worth of HTTP requests to the University of Calgary's Computer Science web server. You'll work with real-world web server log data to extract meaningful insights and demonstrate your Python data analysis skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81debeba",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e728b83",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* Work in the cells below - You can add as many cells as needed for data loading, cleaning, and exploration\n",
    "* Import required libraries\n",
    "* Implement data loading and cleaning - Create functions to download, parse, and clean the log data\n",
    "* Explore the data - Understand the structure and identify any data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c05313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can write your code here for data loading, cleaning, and exploration. Add cells as necessary.\n",
    "\n",
    "\n",
    "import gzip\n",
    "import urllib.request\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Download & Read\n",
    "url = \"ftp://ita.ee.lbl.gov/traces/calgary_access_log.gz\"\n",
    "local_file = \"calgary_access_log.gz\"\n",
    "urllib.request.urlretrieve(url, local_file)\n",
    "\n",
    "lines = []\n",
    "with gzip.open(local_file, 'rt', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        lines.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0baba",
   "metadata": {},
   "source": [
    " 2. Parse & Clean Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b42c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid entries after parsing: 722270\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Regex for Apache Common Log Format\n",
    "LOG_RE = re.compile(\n",
    "    r'(?P<remotehost>\\S+) (\\S+) (\\S+) \\[(?P<dt>[^\\]]+)\\] '\n",
    "    r'\"(?P<request>[^\"]+)\" (?P<status>\\d{3}) (?P<bytes>\\S+)'\n",
    ")\n",
    "\n",
    "def parse_line(line: str):\n",
    "    m = LOG_RE.match(line)\n",
    "    if not m:\n",
    "        return None  # malformed\n",
    "    d = m.groupdict()\n",
    "    \n",
    "    # 1) Parse timestamp → datetime\n",
    "    try:\n",
    "        d['datetime'] = datetime.strptime(d['dt'], \"%d/%b/%Y:%H:%M:%S %z\")\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "    # 2) Split request into method, resource, protocol\n",
    "    parts = d['request'].split()\n",
    "    if len(parts) != 3:\n",
    "        return None\n",
    "    d['method'], resource, d['protocol'] = parts\n",
    "    d['filename'] = resource.split(\"/\")[-1] if \"/\" in resource else resource\n",
    "    \n",
    "    # 3) Convert numeric fields\n",
    "    d['status'] = int(d['status'])\n",
    "    d['bytes']  = int(d['bytes']) if d['bytes'].isdigit() else 0\n",
    "    \n",
    "    # 4) Extract extension\n",
    "    d['extension'] = d['filename'].split(\".\")[-1] if \".\" in d['filename'] else \"\"\n",
    "    \n",
    "    return d\n",
    "\n",
    "# Build parsed list, filtering out None\n",
    "parsed_logs = [rec for rec in (parse_line(l) for l in lines) if rec]\n",
    "print(f\"Valid entries after parsing: {len(parsed_logs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5f13eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remotehost                       object\n",
      "dt                               object\n",
      "request                          object\n",
      "status                            int64\n",
      "bytes                             int64\n",
      "datetime      datetime64[ns, UTC-06:00]\n",
      "method                           object\n",
      "protocol                         object\n",
      "filename                         object\n",
      "extension                        object\n",
      "date_str                         object\n",
      "hour                              int32\n",
      "dtype: object\n",
      "                   datetime     date_str  hour\n",
      "0 1994-10-24 13:41:41-06:00  24-Oct-1994    13\n",
      "1 1994-10-24 13:41:41-06:00  24-Oct-1994    13\n",
      "2 1994-10-24 13:43:13-06:00  24-Oct-1994    13\n",
      "3 1994-10-24 13:43:14-06:00  24-Oct-1994    13\n",
      "4 1994-10-24 13:43:15-06:00  24-Oct-1994    13\n"
     ]
    }
   ],
   "source": [
    "# Force-cast to datetime64\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "# Now you can use .dt\n",
    "df['date_str'] = df['datetime'].dt.strftime('%d-%b-%Y')\n",
    "df['hour']     = df['datetime'].dt.hour\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df[['datetime','date_str','hour']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db784a",
   "metadata": {},
   "source": [
    "## Convert to DataFrame & Final Clean‑up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd602570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaT count in datetime: 290436\n",
      "remotehost                       object\n",
      "dt                               object\n",
      "request                          object\n",
      "status                            int64\n",
      "bytes                             int64\n",
      "datetime      datetime64[ns, UTC-06:00]\n",
      "method                           object\n",
      "protocol                         object\n",
      "filename                         object\n",
      "extension                        object\n",
      "date_str                         object\n",
      "hour                              int32\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>remotehost</th>\n",
       "      <th>dt</th>\n",
       "      <th>request</th>\n",
       "      <th>status</th>\n",
       "      <th>bytes</th>\n",
       "      <th>datetime</th>\n",
       "      <th>method</th>\n",
       "      <th>protocol</th>\n",
       "      <th>filename</th>\n",
       "      <th>extension</th>\n",
       "      <th>date_str</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>local</td>\n",
       "      <td>24/Oct/1994:13:41:41 -0600</td>\n",
       "      <td>GET index.html HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>1994-10-24 13:41:41-06:00</td>\n",
       "      <td>GET</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>index.html</td>\n",
       "      <td>html</td>\n",
       "      <td>24-Oct-1994</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>local</td>\n",
       "      <td>24/Oct/1994:13:41:41 -0600</td>\n",
       "      <td>GET 1.gif HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1210</td>\n",
       "      <td>1994-10-24 13:41:41-06:00</td>\n",
       "      <td>GET</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>1.gif</td>\n",
       "      <td>gif</td>\n",
       "      <td>24-Oct-1994</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>local</td>\n",
       "      <td>24/Oct/1994:13:43:13 -0600</td>\n",
       "      <td>GET index.html HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>3185</td>\n",
       "      <td>1994-10-24 13:43:13-06:00</td>\n",
       "      <td>GET</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>index.html</td>\n",
       "      <td>html</td>\n",
       "      <td>24-Oct-1994</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>local</td>\n",
       "      <td>24/Oct/1994:13:43:14 -0600</td>\n",
       "      <td>GET 2.gif HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>2555</td>\n",
       "      <td>1994-10-24 13:43:14-06:00</td>\n",
       "      <td>GET</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>2.gif</td>\n",
       "      <td>gif</td>\n",
       "      <td>24-Oct-1994</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>local</td>\n",
       "      <td>24/Oct/1994:13:43:15 -0600</td>\n",
       "      <td>GET 3.gif HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>36403</td>\n",
       "      <td>1994-10-24 13:43:15-06:00</td>\n",
       "      <td>GET</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>3.gif</td>\n",
       "      <td>gif</td>\n",
       "      <td>24-Oct-1994</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  remotehost                          dt                  request  status  \\\n",
       "0      local  24/Oct/1994:13:41:41 -0600  GET index.html HTTP/1.0     200   \n",
       "1      local  24/Oct/1994:13:41:41 -0600       GET 1.gif HTTP/1.0     200   \n",
       "2      local  24/Oct/1994:13:43:13 -0600  GET index.html HTTP/1.0     200   \n",
       "3      local  24/Oct/1994:13:43:14 -0600       GET 2.gif HTTP/1.0     200   \n",
       "4      local  24/Oct/1994:13:43:15 -0600       GET 3.gif HTTP/1.0     200   \n",
       "\n",
       "   bytes                  datetime method  protocol    filename extension  \\\n",
       "0    150 1994-10-24 13:41:41-06:00    GET  HTTP/1.0  index.html      html   \n",
       "1   1210 1994-10-24 13:41:41-06:00    GET  HTTP/1.0       1.gif       gif   \n",
       "2   3185 1994-10-24 13:43:13-06:00    GET  HTTP/1.0  index.html      html   \n",
       "3   2555 1994-10-24 13:43:14-06:00    GET  HTTP/1.0       2.gif       gif   \n",
       "4  36403 1994-10-24 13:43:15-06:00    GET  HTTP/1.0       3.gif       gif   \n",
       "\n",
       "      date_str  hour  \n",
       "0  24-Oct-1994    13  \n",
       "1  24-Oct-1994    13  \n",
       "2  24-Oct-1994    13  \n",
       "3  24-Oct-1994    13  \n",
       "4  24-Oct-1994    13  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(parsed_logs)\n",
    "\n",
    "# Ensure datetime dtype\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "# Flag or drop any remaining NaT\n",
    "print(\"NaT count in datetime:\", df['datetime'].isna().sum())\n",
    "df = df.dropna(subset=['datetime'])\n",
    "\n",
    "# Add helper columns\n",
    "df['date_str'] = df['datetime'].dt.strftime('%d-%b-%Y')\n",
    "df['hour']     = df['datetime'].dt.hour\n",
    "\n",
    "# Final look\n",
    "print(df.dtypes)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89171a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e491a4e",
   "metadata": {},
   "source": [
    "## ⚠️ IMPORTANT: Template Questions Section\n",
    "**DO NOT MODIFY THE TEMPLATE BELOW THIS POINT**\n",
    "\n",
    "The following section contains the assessment questions. You may add cells above this section for data loading, cleaning, and exploration, but do not modify the function signatures or structure of the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da5c6e",
   "metadata": {},
   "source": [
    "## Part 2: Analysis Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f1ce65",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* Implement each function according to its docstring specifications\n",
    "* Use the cleaned data you prepared in Part 1\n",
    "* Ensure your functions return the exact data types specified\n",
    "* Test your functions to verify they work correctly\n",
    "* You may add helper functions, but keep the main function signatures unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff13fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q1: Count of total log records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b4569",
   "metadata": {},
   "source": [
    "Q1: Count of total log records\n",
    "\n",
    "○ Description: Count the total number of HTTP requests in the log file.\n",
    "\n",
    "Each line represents one log entry.\n",
    "\n",
    "○ Return Type: int\n",
    "\n",
    "○ Example: 123456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6264dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1:\n",
      "726739\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "def total_log_records(filepath: str = \"calgary_access_log.gz\") -> int:\n",
    "    \"\"\"\n",
    "    Q1: Count of total log records.\n",
    "\n",
    "    Objective:\n",
    "        Determine the total number of HTTP log entries in the dataset.\n",
    "        Each line in the log file represents one HTTP request.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the .gz log file.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of log entries.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    try:\n",
    "        with gzip.open(filepath, \"rt\", errors=\"ignore\") as f:\n",
    "            for _ in f:\n",
    "                count += 1\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Log file not found: {filepath}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading log file: {e}\")\n",
    "    return count\n",
    "\n",
    "# Usage\n",
    "answer1 = total_log_records()\n",
    "print(\"Answer 1:\")\n",
    "print(answer1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5141e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q2: Count of unique hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dd5c7",
   "metadata": {},
   "source": [
    "Q2: Count of unique hosts\n",
    "\n",
    "○ Description: Determine the number of distinct hosts (IP addresses or\n",
    "domain names) that accessed the server.\n",
    "\n",
    "○ Return Type: int\n",
    "\n",
    "○ Example: 8567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcbccae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 2:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "def unique_host_count(filepath: str = \"calgary_access_log.gz\") -> int:\n",
    "    \"\"\"\n",
    "    Q2: Count of unique hosts.\n",
    "\n",
    "    Objective:\n",
    "        Determine how many distinct hosts accessed the server.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the .gz log file.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of unique hosts.\n",
    "    \"\"\"\n",
    "    hosts = set()\n",
    "    try:\n",
    "        with gzip.open(filepath, \"rt\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                parts = line.split()\n",
    "                if parts:\n",
    "                    hosts.add(parts[0])\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Log file not found: {filepath}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading log file: {e}\")\n",
    "    return len(hosts)\n",
    "\n",
    "# Usage\n",
    "answer2 = unique_host_count()\n",
    "print(\"Answer 2:\")\n",
    "print(answer2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c224d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q3: Date-wise unique filename counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86c1ce",
   "metadata": {},
   "source": [
    "Date-wise unique filename counts\n",
    "\n",
    "○ Description: For each date, count how many unique filenames were\n",
    "requested.\n",
    "\n",
    "○ Return Type: dict[str, int]\n",
    "\n",
    "○ Format: { '01-Jul-1995': 123, '02-Jul-1995': 150 }\n",
    "\n",
    "○ Note: Date format must be 'dd-MMM-yyyy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac11c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 3:\n",
      "{'24-Oct-1994': 228, '25-Oct-1994': 319, '26-Oct-1994': 377, '27-Oct-1994': 384, '28-Oct-1994': 399, '29-Oct-1994': 254, '30-Oct-1994': 236, '31-Oct-1994': 361, '01-Nov-1994': 412, '02-Nov-1994': 427, '03-Nov-1994': 459, '04-Nov-1994': 402, '05-Nov-1994': 193, '06-Nov-1994': 219, '07-Nov-1994': 364, '08-Nov-1994': 266, '09-Nov-1994': 335, '10-Nov-1994': 356, '11-Nov-1994': 297, '12-Nov-1994': 173, '13-Nov-1994': 186, '14-Nov-1994': 329, '15-Nov-1994': 324, '16-Nov-1994': 391, '17-Nov-1994': 440, '18-Nov-1994': 403, '19-Nov-1994': 195, '20-Nov-1994': 263, '21-Nov-1994': 335, '22-Nov-1994': 351, '23-Nov-1994': 322, '24-Nov-1994': 365, '25-Nov-1994': 323, '26-Nov-1994': 221, '27-Nov-1994': 187, '28-Nov-1994': 341, '29-Nov-1994': 448, '30-Nov-1994': 354, '01-Dec-1994': 271, '02-Dec-1994': 323, '03-Dec-1994': 189, '04-Dec-1994': 212, '05-Dec-1994': 351, '06-Dec-1994': 297, '07-Dec-1994': 383, '08-Dec-1994': 346, '09-Dec-1994': 372, '10-Dec-1994': 150, '11-Dec-1994': 202, '12-Dec-1994': 402, '13-Dec-1994': 380, '14-Dec-1994': 303, '15-Dec-1994': 281, '16-Dec-1994': 383, '17-Dec-1994': 296, '18-Dec-1994': 323, '19-Dec-1994': 365, '20-Dec-1994': 316, '21-Dec-1994': 280, '22-Dec-1994': 267, '23-Dec-1994': 157, '24-Dec-1994': 66, '25-Dec-1994': 71, '26-Dec-1994': 101, '27-Dec-1994': 201, '28-Dec-1994': 129, '29-Dec-1994': 128, '30-Dec-1994': 131, '31-Dec-1994': 94, '01-Jan-1995': 88, '02-Jan-1995': 141, '03-Jan-1995': 310, '04-Jan-1995': 324, '05-Jan-1995': 284, '06-Jan-1995': 206, '07-Jan-1995': 153, '08-Jan-1995': 207, '09-Jan-1995': 364, '10-Jan-1995': 371, '11-Jan-1995': 361, '12-Jan-1995': 460, '13-Jan-1995': 427, '14-Jan-1995': 187, '15-Jan-1995': 217, '16-Jan-1995': 451, '17-Jan-1995': 381, '18-Jan-1995': 394, '19-Jan-1995': 478, '20-Jan-1995': 502, '21-Jan-1995': 258, '22-Jan-1995': 276, '23-Jan-1995': 438, '24-Jan-1995': 423, '25-Jan-1995': 415, '26-Jan-1995': 389, '27-Jan-1995': 445, '28-Jan-1995': 404, '29-Jan-1995': 365, '30-Jan-1995': 581, '31-Jan-1995': 492, '01-Feb-1995': 620, '02-Feb-1995': 522, '03-Feb-1995': 569, '04-Feb-1995': 561, '05-Feb-1995': 483, '06-Feb-1995': 648, '07-Feb-1995': 695, '08-Feb-1995': 627, '09-Feb-1995': 710, '10-Feb-1995': 727, '11-Feb-1995': 358, '12-Feb-1995': 500, '13-Feb-1995': 670, '14-Feb-1995': 705, '15-Feb-1995': 795, '16-Feb-1995': 664, '17-Feb-1995': 567, '18-Feb-1995': 549, '19-Feb-1995': 381, '20-Feb-1995': 494, '21-Feb-1995': 460, '22-Feb-1995': 706, '23-Feb-1995': 601, '24-Feb-1995': 512, '25-Feb-1995': 468, '26-Feb-1995': 391, '27-Feb-1995': 529, '28-Feb-1995': 476, '01-Mar-1995': 582, '02-Mar-1995': 599, '03-Mar-1995': 503, '04-Mar-1995': 402, '05-Mar-1995': 471, '06-Mar-1995': 634, '07-Mar-1995': 762, '08-Mar-1995': 605, '09-Mar-1995': 686, '10-Mar-1995': 702, '11-Mar-1995': 468, '12-Mar-1995': 479, '13-Mar-1995': 856, '14-Mar-1995': 777, '15-Mar-1995': 909, '16-Mar-1995': 601, '17-Mar-1995': 513, '18-Mar-1995': 418, '19-Mar-1995': 360, '20-Mar-1995': 683, '21-Mar-1995': 737, '22-Mar-1995': 616, '23-Mar-1995': 748, '24-Mar-1995': 518, '25-Mar-1995': 614, '26-Mar-1995': 536, '27-Mar-1995': 834, '28-Mar-1995': 849, '29-Mar-1995': 897, '30-Mar-1995': 861, '31-Mar-1995': 814, '01-Apr-1995': 435, '02-Apr-1995': 465, '03-Apr-1995': 795, '04-Apr-1995': 821, '05-Apr-1995': 891, '06-Apr-1995': 678, '07-Apr-1995': 773, '08-Apr-1995': 542, '09-Apr-1995': 623, '10-Apr-1995': 750, '11-Apr-1995': 816, '12-Apr-1995': 887, '13-Apr-1995': 613, '14-Apr-1995': 353, '15-Apr-1995': 418, '16-Apr-1995': 434, '17-Apr-1995': 446, '18-Apr-1995': 452, '19-Apr-1995': 701, '20-Apr-1995': 587, '21-Apr-1995': 713, '22-Apr-1995': 435, '23-Apr-1995': 332, '24-Apr-1995': 528, '25-Apr-1995': 556, '26-Apr-1995': 646, '27-Apr-1995': 616, '28-Apr-1995': 637, '29-Apr-1995': 448, '30-Apr-1995': 277, '01-May-1995': 467, '02-May-1995': 700, '03-May-1995': 589, '04-May-1995': 684, '05-May-1995': 607, '06-May-1995': 517, '07-May-1995': 724, '08-May-1995': 656, '09-May-1995': 774, '10-May-1995': 794, '11-May-1995': 599, '12-May-1995': 469, '13-May-1995': 289, '14-May-1995': 326, '15-May-1995': 584, '16-May-1995': 432, '17-May-1995': 508, '18-May-1995': 528, '19-May-1995': 499, '20-May-1995': 254, '21-May-1995': 288, '22-May-1995': 477, '23-May-1995': 565, '24-May-1995': 489, '25-May-1995': 487, '26-May-1995': 424, '27-May-1995': 244, '28-May-1995': 205, '29-May-1995': 463, '30-May-1995': 553, '31-May-1995': 565, '01-Jun-1995': 590, '02-Jun-1995': 511, '03-Jun-1995': 398, '04-Jun-1995': 353, '05-Jun-1995': 494, '06-Jun-1995': 662, '07-Jun-1995': 485, '08-Jun-1995': 640, '09-Jun-1995': 468, '10-Jun-1995': 328, '11-Jun-1995': 297, '12-Jun-1995': 519, '13-Jun-1995': 465, '14-Jun-1995': 588, '15-Jun-1995': 479, '16-Jun-1995': 529, '17-Jun-1995': 383, '18-Jun-1995': 358, '19-Jun-1995': 612, '20-Jun-1995': 531, '21-Jun-1995': 625, '22-Jun-1995': 630, '23-Jun-1995': 561, '24-Jun-1995': 396, '25-Jun-1995': 569, '26-Jun-1995': 637, '27-Jun-1995': 518, '28-Jun-1995': 573, '29-Jun-1995': 469, '30-Jun-1995': 461, '01-Jul-1995': 380, '02-Jul-1995': 397, '03-Jul-1995': 433, '04-Jul-1995': 610, '05-Jul-1995': 607, '06-Jul-1995': 522, '07-Jul-1995': 428, '08-Jul-1995': 277, '09-Jul-1995': 233, '10-Jul-1995': 502, '11-Jul-1995': 570, '12-Jul-1995': 466, '13-Jul-1995': 499, '14-Jul-1995': 551, '15-Jul-1995': 384, '16-Jul-1995': 299, '17-Jul-1995': 567, '18-Jul-1995': 557, '19-Jul-1995': 471, '20-Jul-1995': 568, '21-Jul-1995': 649, '22-Jul-1995': 444, '23-Jul-1995': 498, '24-Jul-1995': 565, '25-Jul-1995': 586, '26-Jul-1995': 594, '27-Jul-1995': 614, '28-Jul-1995': 564, '29-Jul-1995': 320, '30-Jul-1995': 481, '31-Jul-1995': 621, '01-Aug-1995': 669, '02-Aug-1995': 855, '03-Aug-1995': 581, '04-Aug-1995': 715, '05-Aug-1995': 507, '06-Aug-1995': 446, '07-Aug-1995': 608, '08-Aug-1995': 653, '09-Aug-1995': 698, '10-Aug-1995': 635, '11-Aug-1995': 451, '12-Aug-1995': 340, '13-Aug-1995': 463, '14-Aug-1995': 589, '15-Aug-1995': 481, '16-Aug-1995': 601, '17-Aug-1995': 537, '18-Aug-1995': 491, '19-Aug-1995': 377, '20-Aug-1995': 392, '21-Aug-1995': 631, '22-Aug-1995': 536, '23-Aug-1995': 660, '24-Aug-1995': 577, '25-Aug-1995': 595, '26-Aug-1995': 394, '27-Aug-1995': 436, '28-Aug-1995': 545, '29-Aug-1995': 511, '30-Aug-1995': 590, '31-Aug-1995': 509, '01-Sep-1995': 328, '02-Sep-1995': 349, '03-Sep-1995': 212, '04-Sep-1995': 340, '05-Sep-1995': 411, '06-Sep-1995': 548, '07-Sep-1995': 590, '08-Sep-1995': 752, '09-Sep-1995': 407, '10-Sep-1995': 455, '11-Sep-1995': 717, '12-Sep-1995': 716, '13-Sep-1995': 771, '14-Sep-1995': 716, '15-Sep-1995': 707, '16-Sep-1995': 564, '17-Sep-1995': 466, '18-Sep-1995': 653, '19-Sep-1995': 732, '20-Sep-1995': 831, '21-Sep-1995': 800, '22-Sep-1995': 614, '23-Sep-1995': 498, '24-Sep-1995': 593, '25-Sep-1995': 723, '26-Sep-1995': 867, '27-Sep-1995': 826, '28-Sep-1995': 864, '29-Sep-1995': 837, '30-Sep-1995': 650, '01-Oct-1995': 550, '02-Oct-1995': 869, '03-Oct-1995': 844, '04-Oct-1995': 886, '05-Oct-1995': 842, '06-Oct-1995': 865, '07-Oct-1995': 468, '08-Oct-1995': 513, '09-Oct-1995': 740, '10-Oct-1995': 840, '11-Oct-1995': 717}\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "def datewise_unique_filename_counts(filepath: str = \"calgary_access_log.gz\") -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Q3: Date-wise unique filename counts.\n",
    "\n",
    "    Objective:\n",
    "        For each date, count the number of unique filenames that accessed the server.\n",
    "        The date should be in 'dd-MMM-yyyy' format (e.g., '01-Jul-1995').\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the .gz log file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each date to its count of unique filenames.\n",
    "              Example: {'01-Jul-1995': 123, '02-Jul-1995': 150}\n",
    "    \"\"\"\n",
    "    # Regex to pull out the timestamp and request\n",
    "    log_re = re.compile(\n",
    "        r'\\S+ \\S+ \\S+ \\[(?P<dt>[^\\]]+)\\] \"(?P<req>[^\"]+)\" \\d{3} \\S+'\n",
    "    )\n",
    "    \n",
    "    # Map from date_str to set of filenames\n",
    "    unique_files_per_date: dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "    try:\n",
    "        with gzip.open(filepath, \"rt\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                m = log_re.match(line)\n",
    "                if not m:\n",
    "                    continue\n",
    "                \n",
    "                # Parse timestamp\n",
    "                dt_str = m.group(\"dt\")  # e.g. '24/Oct/1994:14:23:15 -0700'\n",
    "                try:\n",
    "                    dt = datetime.strptime(dt_str, \"%d/%b/%Y:%H:%M:%S %z\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                \n",
    "                date_key = dt.strftime(\"%d-%b-%Y\")  # '24-Oct-1994'\n",
    "                \n",
    "                # Extract filename from request\n",
    "                req_parts = m.group(\"req\").split()\n",
    "                if len(req_parts) != 3:\n",
    "                    continue\n",
    "                resource = req_parts[1]               # e.g. '/index.html'\n",
    "                filename = resource.split(\"/\")[-1]    # e.g. 'index.html'\n",
    "                if filename:\n",
    "                    unique_files_per_date[date_key].add(filename)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Log file not found: {filepath}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing log file: {e}\")\n",
    "\n",
    "    # Convert sets to counts\n",
    "    return {date: len(files) for date, files in unique_files_per_date.items()}\n",
    "\n",
    "\n",
    "# Usage\n",
    "answer3 = datewise_unique_filename_counts()\n",
    "print(\"Answer 3:\")\n",
    "print(answer3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2da36a",
   "metadata": {},
   "source": [
    "### Q4: Number of 404 response codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bfce3a",
   "metadata": {},
   "source": [
    "Q4: Number of 404 response codes\n",
    "\n",
    "○ Description: Count how many HTTP requests resulted in a 404 (Not\n",
    "Found) response.\n",
    "\n",
    "○ Return Type: int\n",
    "\n",
    "○ Example: 3490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0671865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 4:\n",
      "23602\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "def count_404_errors(filepath: str = \"calgary_access_log.gz\") -> int:\n",
    "    \"\"\"\n",
    "    Q4: Number of 404 response codes.\n",
    "\n",
    "    Objective:\n",
    "        Count how many times the HTTP 404 Not Found status appears in the logs.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the .gz log file.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of 404 errors.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    try:\n",
    "        with gzip.open(filepath, \"rt\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                parts = line.split()\n",
    "                # In Apache Common Log Format, status is the second-to-last field\n",
    "                if len(parts) >= 2 and parts[-2] == \"404\":\n",
    "                    count += 1\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Log file not found: {filepath}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading log file: {e}\")\n",
    "    return count\n",
    "\n",
    "# Usage\n",
    "answer4 = count_404_errors()\n",
    "print(\"Answer 4:\")\n",
    "print(answer4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73928d2",
   "metadata": {},
   "source": [
    "### Q5: Top 15 filenames with 404 responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f38bd5",
   "metadata": {},
   "source": [
    "Q5: Top 15 filenames with 404 responses\n",
    "\n",
    "○ Description: Find the 15 most requested URLs that resulted in a 404\n",
    "error, sorted by frequency.\n",
    "\n",
    "○ Return Type: list[tuple[str, int]]\n",
    "\n",
    "○ Format: [('missing.html', 200), ('notfound.gif', 123), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "358f0523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 5:\n",
      "[('index.html', 4694), ('4115.html', 902), ('1611.html', 649), ('5698.xbm', 585), ('710.txt', 408), ('2002.html', 258), ('2177.gif', 193), ('10695.ps', 161), ('6555.html', 153), ('487.gif', 152), ('151.html', 149), ('40.html', 148), ('488.gif', 148), ('3414.gif', 148), ('9678.gif', 142)]\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def top_15_filenames_with_404(filepath: str = \"calgary_access_log.gz\") -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Q5: Top 15 filenames with 404 responses.\n",
    "\n",
    "    Objective:\n",
    "        Identify which requested URLs most frequently resulted in a 404 error.\n",
    "        Return the top 15 filenames sorted by frequency.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the .gz log file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples (filename, count), sorted by count in descending order.\n",
    "              Example: [('missing.html', 200), ...]\n",
    "    \"\"\"\n",
    "    # Regex for parsing lines\n",
    "    log_re = re.compile(\n",
    "        r'\\S+ \\S+ \\S+ \\[[^\\]]+\\] \"(?P<req>[^\"]+)\" (?P<status>\\d{3}) \\S+'\n",
    "    )\n",
    "    \n",
    "    counter = Counter()\n",
    "    try:\n",
    "        with gzip.open(filepath, \"rt\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                m = log_re.match(line)\n",
    "                if not m:\n",
    "                    continue\n",
    "                status = int(m.group(\"status\"))\n",
    "                if status != 404:\n",
    "                    continue\n",
    "                \n",
    "                # Extract resource from request\n",
    "                parts = m.group(\"req\").split()\n",
    "                if len(parts) != 3:\n",
    "                    continue\n",
    "                resource = parts[1]               # e.g. '/path/to/file.html'\n",
    "                filename = resource.split(\"/\")[-1] or resource\n",
    "                counter[filename] += 1\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Log file not found: {filepath}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing log file: {e}\")\n",
    "\n",
    "    return counter.most_common(15)\n",
    "\n",
    "\n",
    "# Usage\n",
    "answer5 = top_15_filenames_with_404()\n",
    "print(\"Answer 5:\")\n",
    "print(answer5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328c88a",
   "metadata": {},
   "source": [
    "### Q6: Top 15 file extension with 404 responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e8bf2b",
   "metadata": {},
   "source": [
    "Top 15 file extensions with 404 responses\n",
    "\n",
    "○ Description: Identify the file extensions (like .html, .jpg) that caused the\n",
    "most 404 errors.\n",
    "\n",
    "○ Return Type: list[tuple[str, int]]\n",
    "\n",
    "○ Format: [('html', 345), ('gif', 220), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0aca8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 6:\n",
      "[('html', 12145), ('gif', 7337), ('xbm', 824), ('ps', 754), ('jpg', 531), ('txt', 508), ('', 337), ('htm', 108), ('cgi', 77), ('com', 45), ('z', 41), ('dvi', 40), ('ca', 36), ('hmtl', 30), ('util', 29)]\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def top_15_ext_with_404(filepath: str = \"calgary_access_log.gz\") -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Q6: Top 15 file extensions with 404 responses.\n",
    "\n",
    "    Objective:\n",
    "        Find which file extensions generated the most 404 errors.\n",
    "        Return the top 15 sorted by number of 404s.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the .gz log file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples (extension, count), sorted by count in descending order.\n",
    "              Example: [('html', 45), ...]\n",
    "    \"\"\"\n",
    "    # Regex to parse the request line and status\n",
    "    log_re = re.compile(\n",
    "        r'\\S+ \\S+ \\S+ \\[[^\\]]+\\] \"(?P<req>[^\"]+)\" (?P<status>\\d{3}) \\S+'\n",
    "    )\n",
    "\n",
    "    ext_counter = Counter()\n",
    "    try:\n",
    "        with gzip.open(filepath, \"rt\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                m = log_re.match(line)\n",
    "                if not m:\n",
    "                    continue\n",
    "                status = int(m.group(\"status\"))\n",
    "                if status != 404:\n",
    "                    continue\n",
    "\n",
    "                # Extract the requested resource path\n",
    "                parts = m.group(\"req\").split()\n",
    "                if len(parts) != 3:\n",
    "                    continue\n",
    "                resource = parts[1]  # e.g. '/path/to/file.html'\n",
    "                filename = resource.split(\"/\")[-1]\n",
    "\n",
    "                # Get extension (text after last dot), or empty string\n",
    "                if \".\" in filename:\n",
    "                    ext = filename.rsplit(\".\", 1)[1].lower()\n",
    "                else:\n",
    "                    ext = \"\"\n",
    "\n",
    "                ext_counter[ext] += 1\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Log file not found: {filepath}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing log file: {e}\")\n",
    "\n",
    "    # Return the 15 most common extensions\n",
    "    return ext_counter.most_common(15)\n",
    "\n",
    "\n",
    "# Usage\n",
    "answer6 = top_15_ext_with_404()\n",
    "print(\"Answer 6:\")\n",
    "print(answer6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52c8ba",
   "metadata": {},
   "source": [
    "### Q7: Total bandwidth transferred per day for the month of July 1995"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e29ff",
   "metadata": {},
   "source": [
    "● Q7: Total bandwidth transferred per day for July 1995\n",
    "\n",
    "○ Description: Sum the bytes transferred per day for July 1995 (exclude\n",
    "missing or '-' byte values).\n",
    "\n",
    "○ Return Type: dict[str, int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45f52d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 7:\n",
      "{'01-Jul-1995': 11349799, '02-Jul-1995': 8656918, '03-Jul-1995': 13596612, '04-Jul-1995': 26573988, '05-Jul-1995': 19541225, '06-Jul-1995': 19755015, '07-Jul-1995': 9427822, '08-Jul-1995': 5403491, '09-Jul-1995': 4660556, '10-Jul-1995': 14917754, '11-Jul-1995': 22507207, '12-Jul-1995': 17367065, '13-Jul-1995': 15989234, '14-Jul-1995': 19186430, '15-Jul-1995': 15773233, '16-Jul-1995': 9016378, '17-Jul-1995': 19601338, '18-Jul-1995': 17099761, '19-Jul-1995': 17851725, '20-Jul-1995': 20752623, '21-Jul-1995': 25491617, '22-Jul-1995': 8136259, '23-Jul-1995': 9593870, '24-Jul-1995': 22308265, '25-Jul-1995': 24561635, '26-Jul-1995': 24995540, '27-Jul-1995': 25969995, '28-Jul-1995': 36460693, '29-Jul-1995': 11700624, '30-Jul-1995': 23189598, '31-Jul-1995': 30730715}\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "def total_bandwidth_per_day(filepath: str = \"calgary_access_log.gz\") -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Q7: Total bandwidth transferred per day for the month of July 1995.\n",
    "\n",
    "    Objective:\n",
    "        Sum the number of bytes transferred per day.\n",
    "        Skip entries where the byte field is missing or '-'.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the .gz log file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each date ('dd-MMM-yyyy') to total bytes transferred.\n",
    "              Example: {'01-Jul-1995': 123456789, ...}\n",
    "    \"\"\"\n",
    "    # Regex to parse timestamp and bytes\n",
    "    log_re = re.compile(\n",
    "        r'\\S+ \\S+ \\S+ \\[(?P<dt>[^\\]]+)\\] \"[^\"]+\" \\d{3} (?P<bytes>\\S+)'\n",
    "    )\n",
    "\n",
    "    # Accumulator for each date\n",
    "    bandwidth_per_day = defaultdict(int)\n",
    "\n",
    "    try:\n",
    "        with gzip.open(filepath, \"rt\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                m = log_re.match(line)\n",
    "                if not m:\n",
    "                    continue\n",
    "\n",
    "                # Parse datetime\n",
    "                dt_str = m.group(\"dt\")  # e.g. '24/Oct/1994:14:23:15 -0700'\n",
    "                try:\n",
    "                    dt = datetime.strptime(dt_str, \"%d/%b/%Y:%H:%M:%S %z\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                # Only July 1995\n",
    "                if dt.year != 1995 or dt.month != 7:\n",
    "                    continue\n",
    "\n",
    "                # Parse bytes field\n",
    "                byte_str = m.group(\"bytes\")\n",
    "                if byte_str.isdigit():\n",
    "                    bandwidth_per_day[dt.strftime(\"%d-%b-%Y\")] += int(byte_str)\n",
    "                # skip '-' or malformed\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Log file not found: {filepath}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing log file: {e}\")\n",
    "\n",
    "    return dict(bandwidth_per_day)\n",
    "\n",
    "# Usage\n",
    "answer7 = total_bandwidth_per_day()\n",
    "print(\"Answer 7:\")\n",
    "print(answer7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc00908",
   "metadata": {},
   "source": [
    "### Q8: Hourly request distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991066c",
   "metadata": {},
   "source": [
    "Q8: Hourly request distribution\n",
    "\n",
    "○ Description: Count how many HTTP requests occurred during each\n",
    "hour (0–23).\n",
    "\n",
    "○ Return Type: dict[int, int]\n",
    "\n",
    "○ Format: { 0: 1200, 1: 900, ..., 23: 670 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a77f3e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly distribution (0–23): {13: 51457, 14: 54562, 15: 50377, 16: 51176, 17: 45060, 18: 33222, 19: 30573, 20: 29691, 21: 27405, 22: 23827, 23: 21883, 0: 18764, 1: 14389, 3: 10901, 4: 9969, 5: 10804, 6: 13059, 7: 16672, 8: 26591, 9: 33987, 10: 43371, 11: 47588, 12: 46814, 2: 12694}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import gzip\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_RE = re.compile(\n",
    "    r'(?P<host>\\S+) \\S+ \\S+ \\[(?P<dt>[^\\]]+)\\] '\n",
    "    r'\"(?P<req>[^\"]+)\" (?P<status>\\d{3}) (?P<bytes>\\S+)'\n",
    ")\n",
    "\n",
    "def hourly_request_distribution(filepath: str = \"calgary_access_log.gz\") -> dict[int, int]:\n",
    "    counts = defaultdict(int)\n",
    "    with gzip.open(filepath, \"rt\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            m = LOG_RE.match(line)\n",
    "            if not m: \n",
    "                continue\n",
    "            try:\n",
    "                dt = datetime.strptime(m.group(\"dt\"), \"%d/%b/%Y:%H:%M:%S %z\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "            counts[dt.hour] += 1\n",
    "    return dict(counts)\n",
    "\n",
    "# Run and display\n",
    "answer8 = hourly_request_distribution()\n",
    "print(\"Hourly distribution (0–23):\", answer8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b7083",
   "metadata": {},
   "source": [
    "### Q9: Top 10 most requested filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50653b76",
   "metadata": {},
   "source": [
    "Q9: Top 10 most requested filenames\n",
    "\n",
    "○ Description: Identify the top 10 most frequently requested filenames,\n",
    "regardless of status code.\n",
    "\n",
    "○ Return Type: list[tuple[str, int]]\n",
    "\n",
    "○ Format: [('index.html', 5678), ('home.gif', 4321), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91168ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 requested filenames: [('index.html', 139528), ('3.gif', 24006), ('2.gif', 23595), ('4.gif', 8018), ('244.gif', 5148), ('5.html', 5010), ('4097.gif', 4874), ('8870.jpg', 4492), ('6733.gif', 4278), ('8472.gif', 3843)]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from collections import Counter\n",
    "\n",
    "def top_10_most_requested_filenames(filepath: str = \"calgary_access_log.gz\") -> list[tuple[str, int]]:\n",
    "    counter = Counter()\n",
    "    with gzip.open(filepath, \"rt\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            m = LOG_RE.match(line)\n",
    "            if not m:\n",
    "                continue\n",
    "            parts = m.group(\"req\").split()\n",
    "            if len(parts) != 3:\n",
    "                continue\n",
    "            resource = parts[1]\n",
    "            filename = resource.split(\"/\")[-1] or resource\n",
    "            counter[filename] += 1\n",
    "    return counter.most_common(10)\n",
    "\n",
    "# Run and display\n",
    "answer9 = top_10_most_requested_filenames()\n",
    "print(\"Top 10 requested filenames:\", answer9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb4778",
   "metadata": {},
   "source": [
    "### Q10: HTTP response code distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7533f188",
   "metadata": {},
   "source": [
    "Q10: HTTP response code distribution\n",
    "\n",
    "○ Description: Count the occurrences of each HTTP response status code\n",
    "(e.g., 200, 404).\n",
    "\n",
    "○ Return Type: dict[int, int]\n",
    "\n",
    "○ Format: { 200: 150000, 404: 3200, 500: 87 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd4453ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code distribution: {200: 568502, 302: 30325, 304: 97792, 404: 23602, 780: 1, 403: 4743, 501: 43, 400: 19, 329: 1, 579: 1, 0: 39, 500: 42, 55124: 1, 506: 1, 9154: 1, 1619: 1, 732: 1, 3207: 1, 8192: 3, 530: 1, 2227: 1, 26048: 1, 1268: 4, 1191: 1, 14374: 1, 899: 1, 2060: 1, 2595: 1, 2881: 5, 2711: 1, 2658: 1, 479: 2, 2830: 1, 887: 3, 768: 1, 1791: 1, 1062: 1, 1377: 1, 527: 1, 4627: 1, 1831: 1, 1018: 1, 1294: 1, 1527: 1, 1926: 1, 1363: 1, 688: 1, 2632: 1, 1113: 1, 489: 1, 288: 1, 2587: 1, 1692: 1, 2574: 1, 2420: 1, 2014: 1, 2344: 1, 11079: 1, 755: 1, 1524: 1, 224: 1, 1611: 1, 2250: 1, 3067: 1, 378: 1, 2972: 1, 1819: 1, 968: 1, 554: 1, 2019: 1, 21569: 1, 8046: 1, 252: 2, 5058: 1, 2470: 1, 1179: 1, 999: 1, 1567: 2, 1267: 1, 10746: 2, 401: 46, 2544: 1, 4878: 1, 10853: 1, 839: 1, 2247: 1, 10814: 2, 7708: 1, 10928: 1, 1682: 1, 19696: 1, 718: 1, 5409: 1, 12053: 1, 2786: 1, 7259: 2, 4308: 1, 1629: 2, 1729: 2, 3818: 1, 1046: 1, 3020: 1, 1596: 1, 322: 1}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from collections import Counter\n",
    "\n",
    "def response_code_distribution(filepath: str = \"calgary_access_log.gz\") -> dict[int, int]:\n",
    "    counter = Counter()\n",
    "    with gzip.open(filepath, \"rt\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rsplit(maxsplit=2)\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            status_str = parts[-2]\n",
    "            if status_str.isdigit():\n",
    "                counter[int(status_str)] += 1\n",
    "    return dict(counter)\n",
    "\n",
    "# Run and display\n",
    "answer10 = response_code_distribution()\n",
    "print(\"Status code distribution:\", answer10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
